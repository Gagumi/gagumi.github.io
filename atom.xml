<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://gagumi.github.io</id>
    <title>Gagumi&apos;s blog</title>
    <updated>2023-05-28T11:43:45.435Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://gagumi.github.io"/>
    <link rel="self" href="https://gagumi.github.io/atom.xml"/>
    <subtitle>“Efficiency is a clever laziness”&lt;/br&gt;&lt;/br&gt;
你好！Hi！こんにちは！Bonjour！Grüß Gott！&lt;/br&gt;&lt;/br&gt;
👋 Welcome to my site. I am a developer, creator, and designer. My blog documents my creative process, shares my experiences, and explores various technologies</subtitle>
    <logo>https://gagumi.github.io/images/avatar.png</logo>
    <icon>https://gagumi.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, Gagumi&apos;s blog</rights>
    <entry>
        <title type="html"><![CDATA[有关动漫网站舆情分析Datalake的实现]]></title>
        <id>https://gagumi.github.io/post/you-guan-dong-man-wang-zhan-yu-qing-fen-xi-datalake-de-shi-xian/</id>
        <link href="https://gagumi.github.io/post/you-guan-dong-man-wang-zhan-yu-qing-fen-xi-datalake-de-shi-xian/">
        </link>
        <updated>2023-05-27T17:39:48.000Z</updated>
        <summary type="html"><![CDATA[<p style="font-size:20px;">之前的博客已经对API有了充分的了解，由于其局限性，除了手动爬取下来的06-23年动画的评论数据以外。本篇所有的实现都是基于实时top50动漫的类型分析和评论分析。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
]]></summary>
        <content type="html"><![CDATA[<p style="font-size:20px;">之前的博客已经对API有了充分的了解，由于其局限性，除了手动爬取下来的06-23年动画的评论数据以外。本篇所有的实现都是基于实时top50动漫的类型分析和评论分析。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
<!-- more -->
<h3 id="训练数据的预处理与模型的训练">训练数据的预处理与模型的训练</h3>
<p>前篇所获得的评论与标记数据，在这里的思路是进行机器学习分类器的训练。常见的方法有（支持向量机SVM，决策树，朴素贝叶斯等）其中支持向量机完成的是二分任务，鉴于本次模型需要分类出（recommend，not recommend和mixed filling）所以选择朴素贝叶斯或是决策树。由于朴素贝叶斯的结果过于离谱，这里选用决策树。</p>
<p>预处理方面，这里的思路是使用NLTK官方的停用词和词形还原器。然后自定义一个文本清洗函数来去除特殊字符，标点符号和数字。</p>
<p>特征表示方面，有三种常见的特征表示方法（词袋模型，TF-IDF，word2vec）。由于哥们对NLP确实不够了解，这里使用暴力的方法，使用三个特征表示都使用然后取最优的暴力方法。</p>
<p>以下是训练代码：</p>
<pre><code class="language-python">import re
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split, cross_val_score
from gensim.models import Word2Vec
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import joblib
from tqdm import tqdm

# 下载NLTK的停用词和词形还原器所需的数据
nltk.download('stopwords')
nltk.download('wordnet')

# 初始化词形还原器和停用词集合
lemmatizer = WordNetLemmatizer()
stopwords = list(stopwords.words('english'))


# 自定义文本清洗函数
def clean_text(text):
    # 去除特殊字符、标点符号和数字
    text = re.sub(r&quot;[^\w\s]&quot;, &quot;&quot;, text)
    text = re.sub(r&quot;\d+&quot;, &quot;&quot;, text)
    # 转换为小写
    text = text.lower()
    return text

# 读取训练数据集
df = pd.read_csv(&quot;standard_dataset.csv&quot;)

# 清洗文本数据
df[&quot;Comment&quot;] = df[&quot;Comment&quot;].apply(clean_text)

# 划分训练集和测试集
X = df[&quot;Comment&quot;]
y = df[&quot;Recommendation&quot;]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 特征表示方法1: 词袋模型
count_vectorizer = CountVectorizer(stop_words=stopwords)
X_count = count_vectorizer.fit_transform(X)

# 保存CountVectorizer
count_vectorizer_filename = &quot;count_vectorizer.pkl&quot;
joblib.dump(count_vectorizer, count_vectorizer_filename)
print(f&quot;CountVectorizer saved as {count_vectorizer_filename}&quot;)

# 特征表示方法2: TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
X_tfidf = tfidf_vectorizer.fit_transform(X)

# 保存TfidfVectorizer
tfidf_vectorizer_filename = &quot;tfidf_vectorizer.pkl&quot;
joblib.dump(tfidf_vectorizer, tfidf_vectorizer_filename)
print(f&quot;TfidfVectorizer saved as {tfidf_vectorizer_filename}&quot;)

# 特征表示方法3: Word2Vec
sentences = [nltk.word_tokenize(text) for text in X]
word2vec_model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)
X_word2vec = [word2vec_model.wv[nltk.word_tokenize(text)].mean(axis=0) for text in X]

# 保存Word2Vec模型
word2vec_model_filename = &quot;word2vec_model.pkl&quot;
word2vec_model.save(word2vec_model_filename)
print(f&quot;Word2Vec model saved as {word2vec_model_filename}&quot;)

# 构建决策树模型
decision_tree = DecisionTreeClassifier()

# 交叉验证比较不同特征表示方法的性能
methods = [(&quot;CountVectorizer&quot;, X_count), (&quot;TF-IDF&quot;, X_tfidf), (&quot;Word2Vec&quot;, X_word2vec)]
best_method = None
best_score = 0.0

for method_name, X_method in tqdm(methods):
    scores = cross_val_score(decision_tree, X_method, y, cv=5)
    avg_score = scores.mean()
    print(f&quot;{method_name} - Average Score: {avg_score}&quot;)
    
    if avg_score &gt; best_score:
        best_score = avg_score
        best_method = method_name

print(f&quot;\nBest Method: {best_method}&quot;)

# 使用最优特征表示方法进行模型训练和测试
if best_method == &quot;CountVectorizer&quot;:
    X_train_best = count_vectorizer.transform(X_train)
    X_test_best = count_vectorizer.transform(X_test)
elif best_method == &quot;TF-IDF&quot;:
    X_train_best = tfidf_vectorizer.transform(X_train)
    X_test_best = tfidf_vectorizer.transform(X_test)
elif best_method == &quot;Word2Vec&quot;:
    X_train_best = [word2vec_model.wv[nltk.word_tokenize(text)].mean(axis=0) for text in X_train]
    X_test_best = [word2vec_model.wv[nltk.word_tokenize(text)].mean(axis=0) for text in X_test]

decision_tree.fit(X_train_best, y_train)
y_pred = decision_tree.predict(X_test_best)

# 输出模型评估参数
classification_report = classification_report(y_test, y_pred)
print(f&quot;\nClassification Report ({best_method}):&quot;)
print(classification_report)

# 保存决策树模型
decision_tree_filename = &quot;decision_tree_model.pkl&quot;
joblib.dump(decision_tree, decision_tree_filename)
print(f&quot;Decision tree model saved as {decision_tree_filename}&quot;)
</code></pre>
<p>这里做出解释：为什么模型的训练这一步不也在spark上进行？</p>
<p>因为使用的google dataproc spark平台，不知道为什么读取不了哥们在google储存桶里上传的训练集，另外由于使用的是试用的丐版，貌似cpu资源也不够训练一个决策树模型。故而决策树模型的训练在本地完成。</p>
<p>完成训练之后准备工作完成，正式开始datalake的搭建。</p>
<h3 id="思路总结">思路总结</h3>
<p>通过airflow DAG控制三个任务：</p>
<p>1.获取myanimelist排行前五十动漫的基本信息（最重要的是ID，类型），传入kafka的topic：'all_anime_tops'</p>
<p>2.使用spark做数据处理（作为kafka 第一步topic的消费者），获得这些动漫的所有类型标签封装在json文件中，传入到kafka的topic：'all_anime_genres_counts'</p>
<p>3.获取kitsu排行前五十动漫的评论信息（最重要的是ID和评论（评论始终取最新前五条））载入本地预训练好的决策树模型，获得这些评论的语义封装成json，传入到kafka的topic：'prediction_topics'</p>
<p>4.elastic消费2 3 步的kafka topic，使用kibana做实时可视化。</p>
<p>接下里开始实践。</p>
<h3 id="平台的准备与注册">平台的准备与注册</h3>
<p>本次项目所使用的所有工具（airflow，spark，kafka，elk）均是云端平台，使用API key进行连接。所有平台均有免费白嫖的说法。</p>
<p>分别对应 google cloud的cloud composer（airflow），dataproc 集群（spark）；confluent cloud（kafka）；elk（Elasticsearch、Logstash 和 Kibana）</p>
<p>平台的注册都比较简单，这里不多赘述。</p>
<p>进入google平台后，记得新建一个项目。这里我的项目如下：</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528113408.png" alt="" loading="lazy"></figure>
<h3 id="需要的api-key获取以及kafka-topic连接器的建立">需要的API key获取以及kafka topic，连接器的建立</h3>
<p>登录网站https://confluent.cloud/login，注册获得试用资格。</p>
<p>创建一个新的环境和cluster（一步一步跟着引导走即可）</p>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528120136.png" alt="" loading="lazy"></figure>
<p>进入这个集群，点击左侧API key，右上角add一个新key，下载下来保存好。</p>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528120309.png" alt="" loading="lazy"></figure>
<p>接下来点击topics，我们需要创建之前思路里提到的三个topic。</p>
<p>建立之前，根据我们实验课所学，我们需要先建立elk相关物件。以消费这些topic做到可视化。</p>
<p>建立集群之后，我们会被要求下载api key和密码，保存好。</p>
<p>然后点击左侧：<br>
<img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528120803.png" alt="" loading="lazy"></p>
<p>进入deploment，在这里单击复制获得elk的链接（https://biganime.es.us-central1.gcp.cloud.es.io）</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528120914.png" alt="" loading="lazy"></figure>
<p>接下来回到kafka我们可以创建topic了</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528121519.png" alt="" loading="lazy"></figure>
<p>输入topic名字，我这里依次创建了思路里的三个topic。（'all_anime_tops'，'all_anime_genres_counts'，prediction_topics）</p>
<p>接下来创建连接器（类型选elk）与elk链接以实现可视化：</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528124818.png" alt="" loading="lazy"></figure>
<p>按照上图创建，注意ignore掉key和schema，一般来说是不建议这样的，但是不忽略会产生我无法解决的问题。</p>
<h3 id="airflow相关">airflow相关</h3>
<p>在创建composer（https://cloud.google.com/composer）的时候，可以进行选择:</p>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528113449.png" alt="" loading="lazy"></figure>
<p>composer2我始终无法成功创建。故选择的是composer1.</p>
<p>创建之后进入界面：</p>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528113634.png" alt="" loading="lazy"></figure>
<p>首先我们需要安装一个额外的包，来保证airflow和kafka的通讯，点击pypi软件包，安装confluent-kafka。</p>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528113840.png" alt="" loading="lazy"></figure>
<p>接下来打开DAG文件夹，我们的DAG文件就需要放入其中。</p>
<figure data-type="image" tabindex="10"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528114132.png" alt="" loading="lazy"></figure>
<p>这里我在DAG目录的上一级目录新建了一个文件夹，在里面存放了所有会用的东西（包括spark运行所需的jar包，spark安装后的初始化sh命令，spark链接kafka所需要的conf凭证，spark任务py文件，预训练好的模型），将这个gs储存桶当作了一个简易云盘使用。需要调用其中数据时，就可以通过：</p>
<pre><code class="language-html">gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/file
</code></pre>
<p>调用。</p>
<figure data-type="image" tabindex="11"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528115648.png" alt="" loading="lazy"></figure>
<h3 id="spark相关">spark相关</h3>
<p>这一部分时卡住我最久的部分，google dataproc的环境问题有点多。</p>
<p>首先对于该集群的搭建，我使用了命令行的搭建方式。</p>
<p>当我们启用了google dataproc后，我们来到其控制台界面：</p>
<figure data-type="image" tabindex="12"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528115550.png" alt="" loading="lazy"></figure>
<p>单击这里即可进入控制台。不过在此之前我们需要进行一些准备。</p>
<p>首先建议使用的时20年的ubuntu环境，虽说不是最新的，但是个人认为比较稳定。</p>
<p>使用vscode等IDE我们新建一个文件jaas.conf（kafka连接凭证）：</p>
<pre><code class="language-java">KafkaClient {
  org.apache.kafka.common.security.plain.PlainLoginModule required
  username=&quot;4GLFTYVSO5V7MJ6B&quot;
  password=&quot;71JNm1voEHT/NBq8BtAVVLfs8nhmnfcyxHJFCLZLwFPjOfJ9rJ0On4DNnMfun97c&quot;;
};
</code></pre>
<p>这里填上你的kafka API key。</p>
<p>再创建一个sh（copy_jaas.sh）来控制dataproc spark集群创建时需要完成的工作（导入kafka支持包，导入预训练好的模型，移动kafka链接凭证）</p>
<pre><code class="language-sh">#!/bin/bash

# 创建模型存储目录
mkdir -p /home/model/

# 下载模型到模型存储目录
gsutil cp gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/decision_tree_model.pkl /home/model/
gsutil cp gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/count_vectorizer.pkl /home/model/

# 安装库
/opt/conda/default/bin/pip install langdetect
/opt/conda/default/bin/pip install confluent_kafka
/opt/conda/default/bin/pip install requests
/opt/conda/default/bin/pip install joblib
/opt/conda/default/bin/pip install scikit-learn

# 下载 jaas.conf 文件
gsutil cp gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/jaas.conf /etc/
</code></pre>
<p>接下来回到dataproc集群，在终端输入：</p>
<pre><code class="language-shell">gcloud dataproc clusters create cluster-biganime     --region us-west4     --zone us-west4-b     --single-node     --master-machine-type n2-standard-4     --master-boot-disk-size 500     --image-version 1.5-ubuntu18     --project biganime     --initialization-actions gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/copy_jaas.sh
</code></pre>
<p>回车创建一个新的集群。</p>
<h3 id="dag内容">DAG内容</h3>
<p>以下时DAG文件airflow_top50_test.py的内容：</p>
<pre><code class="language-python">from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.contrib.operators.dataproc_operator import DataProcPySparkOperator
from datetime import datetime, timedelta
from requests.auth import HTTPBasicAuth
import requests
import json
import time
from confluent_kafka import SerializingProducer
from confluent_kafka.serialization import StringSerializer

def send_anime_top_to_kafka():
    # 清空 Elasticsearch 索引
    indices_to_delete = [&quot;all_anime_tops&quot;, &quot;all_anime_genres_counts&quot;]

    for index in indices_to_delete:
        response = requests.delete(
            f'https://biganime.es.us-central1.gcp.cloud.es.io/{index}',
            auth=HTTPBasicAuth('elastic', '8wt3nRsnx1IufleEMs8CIOYL')
        )
        if response.status_code == 200:
            print(f&quot;Index {index} has been deleted successfully&quot;)
        elif response.status_code == 404:
            print(f&quot;Index {index} did not exist&quot;)
        else:
            print(f&quot;Error occurred while deleting index {index}&quot;)

    producer_conf = {
        'bootstrap.servers': 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092',
        'security.protocol': 'SASL_SSL',
        'sasl.mechanisms': 'PLAIN',
        'sasl.username': '4GLFTYVSO5V7MJ6B',
        'sasl.password': '71JNm1voEHT/NBq8BtAVVLfs8nhmnfcyxHJFCLZLwFPjOfJ9rJ0On4DNnMfun97c',
        'key.serializer': StringSerializer('utf_8'),
        'value.serializer': StringSerializer('utf_8')
    }
    producer = SerializingProducer(producer_conf)

    for page in [1, 2]:
        response = requests.get(f&quot;https://api.jikan.moe/v4/top/anime?page={page}&quot;)
        top_anime = response.json()

        for anime in top_anime['data']:
            response = requests.get(f&quot;https://api.jikan.moe/v4/anime/{anime['mal_id']}&quot;)
            anime_detail = response.json()

            message_key = anime_detail['data']['title']
            print(message_key)
            message_value = {
                &quot;title&quot;: message_key,
                &quot;genres&quot;: [genre['name'] for genre in anime_detail['data']['genres']],
                &quot;timestamp&quot;: int(time.time())  # 添加当前时间戳
            }
            message_value = json.dumps(message_value)

            producer.produce(topic='all_anime_tops', key=message_key, value=message_value)

            time.sleep(1)

    producer.flush()

# 创建 Airflow DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 5, 24),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'send_anime_top_to_kafka',
    default_args=default_args,
    description='A simple DAG to send anime top to Kafka',
    #schedule_interval='0 7,14 * * *',  # Run at 7AM and 2PM every day
    schedule_interval=None,  # No schedule, only run when manually triggered
    catchup=False
)

t1 = PythonOperator(
    task_id='send_anime_top_to_kafka',
    python_callable=send_anime_top_to_kafka,
    dag=dag,
)

pyspark_job1 = DataProcPySparkOperator(
    task_id='run_pyspark_on_anime_top50_pyspark',
    main='gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/anime_top50_pyspark.py',
    dataproc_properties={
        'spark.executor.extraJavaOptions': '-Djava.security.auth.login.config=/etc/jaas.conf',
        'spark.driver.extraJavaOptions': '-Djava.security.auth.login.config=/etc/jaas.conf'
    },
    dataproc_jars=[
        'gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/spark-sql-kafka-0-10_2.12-2.4.8.jar',
        'gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/kafka-clients-0.10.2.2.jar'
    ],
    cluster_name='cluster-biganime',
    region='us-west4',
    project_id='biganime',
    dag=dag
)

pyspark_job2 = DataProcPySparkOperator(
    task_id='run_pyspark_on_anime_sentiment_prediction',
    main='gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/anime_sentiment_prediction.py',
    dataproc_properties={
        'spark.executor.extraJavaOptions': '-Djava.security.auth.login.config=/etc/jaas.conf',
        'spark.driver.extraJavaOptions': '-Djava.security.auth.login.config=/etc/jaas.conf'
    },
    dataproc_jars=[
        'gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/spark-sql-kafka-0-10_2.12-2.4.8.jar',
        'gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/kafka-clients-0.10.2.2.jar'
    ],
    cluster_name='cluster-biganime',
    region='us-west4',
    project_id='biganime',
    dag=dag
)

t1 &gt;&gt; [pyspark_job1, pyspark_job2]  # 将 PySpark 任务添加到 DAG 中，并行执行

</code></pre>
<p>其中，kafka的bootstrap.servers可以在如下地方获取：</p>
<figure data-type="image" tabindex="13"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528130615.png" alt="" loading="lazy"></figure>
<p>另外，两个pyspark_job启动时所需要的jar包是需要对应spark版本自行下载并上传的。</p>
<pre><code class="language-python">dag = DAG(
    'send_anime_top_to_kafka',
    default_args=default_args,
    description='A simple DAG to send anime top to Kafka',
    #schedule_interval='0 7,14 * * *',  # Run at 7AM and 2PM every day
    schedule_interval=None,  # No schedule, only run when manually triggered
    catchup=False
)
</code></pre>
<p>这里我也设置的是DAG触发一次执行一次，而不是真正按照一天发送两次（方便测试）</p>
<p>接下来是在spark上运行的两个py</p>
<p>anime_top50_pyspark.py（完成了从kafka读取数据进行处理后再传回新的topic的过程）</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, from_json, col, trim
from pyspark.sql.types import StructType, StructField, StringType, LongType  # 添加这行导入语句


spark = SparkSession.builder.appName(&quot;KafkaToKafka&quot;).getOrCreate()

# Define the schema of the input data
schema = StructType([
    StructField(&quot;title&quot;, StringType(), True),
    StructField(&quot;genres&quot;, StringType(), True),
    StructField(&quot;timestamp&quot;, LongType(), True)
])

# 从 Kafka 读取数据
df = spark \
  .readStream \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;pkc-6ojv2.us-west4.gcp.confluent.cloud:9092&quot;) \
  .option(&quot;subscribe&quot;, &quot;all_anime_tops&quot;) \
  .option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;) \
  .option(&quot;kafka.sasl.mechanism&quot;, &quot;PLAIN&quot;) \
  .option(&quot;kafka.sasl.username&quot;, &quot;4GLFTYVSO5V7MJ6B&quot;) \
  .option(&quot;failOnDataLoss&quot;, False) \
  .option(&quot;kafka.sasl.password&quot;, &quot;71JNm1voEHT/NBq8BtAVVLfs8nhmnfcyxHJFCLZLwFPjOfJ9rJ0On4DNnMfun97c&quot;) \
  .load()


# Extract the value of the messages and parse the JSON
df = df.select(from_json(col(&quot;value&quot;).cast(&quot;string&quot;), schema).alias(&quot;data&quot;))

# 处理 genres，去除两端的引号和空格
genres_df = df.select(explode(split(df.data.genres, &quot;,&quot;)).alias(&quot;genre&quot;))
genres_df = genres_df.withColumn(&quot;genre&quot;, trim(genres_df.genre))

# 将结果转为 JSON 格式，其中每条记录包含 genre
genres_df_json = genres_df.selectExpr(&quot;to_json(struct(*)) AS value&quot;)

# 将结果写回 Kafka
query = genres_df_json \
  .writeStream \
  .outputMode(&quot;update&quot;) \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;pkc-6ojv2.us-west4.gcp.confluent.cloud:9092&quot;) \
  .option(&quot;topic&quot;, &quot;all_anime_genres_counts&quot;) \
  .option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;) \
  .option(&quot;kafka.sasl.mechanism&quot;, &quot;PLAIN&quot;) \
  .option(&quot;kafka.sasl.username&quot;, &quot;4GLFTYVSO5V7MJ6B&quot;) \
  .option(&quot;kafka.sasl.password&quot;, &quot;71JNm1voEHT/NBq8BtAVVLfs8nhmnfcyxHJFCLZLwFPjOfJ9rJ0On4DNnMfun97c&quot;) \
  .option(&quot;checkpointLocation&quot;, &quot;gs://europe-west1-biganime-1990c8ad-bucket/checkpoints/&quot;) \
  .start() 

query.awaitTermination(1000)

</code></pre>
<p>anime_sentiment_prediction.py（完成了实时从kitsu上抓取数据并进行决策数分析的过程）</p>
<pre><code class="language-python">from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
from confluent_kafka import SerializingProducer
from confluent_kafka.serialization import StringSerializer
import requests
import joblib
import re
from datetime import datetime
import json  # Import json module

# 函数：将ISO 8601格式的字符串转换为时间戳
def get_timestamp_from_iso8601(iso8601_str):
    date_time_obj = datetime.strptime(iso8601_str, '%Y-%m-%dT%H:%M:%S.%fZ')
    timestamp = datetime.timestamp(date_time_obj)
    return str(timestamp)

# 函数：清洗文本
def clean_text(text):
    text = re.sub(r&quot;[^\w\s]&quot;, &quot;&quot;, text)
    text = re.sub(r&quot;\d+&quot;, &quot;&quot;, text)
    text = text.lower()
    return text

# 函数：预测情绪
def predict_sentiment(text):
    text = clean_text(text)
    vectorized_text = count_vectorizer.transform([text]).toarray()
    prediction = decision_tree.predict(vectorized_text)[0]

    word_count = len(text.split())
    if word_count &lt; 10:
        return &quot;0&quot;  # Mixed Feelings
    elif word_count &gt; 20:
        return &quot;-1&quot;  # Not Recommended
    else:
        if prediction == &quot;Recommended&quot;:
            return &quot;1&quot;
        else:
            return &quot;0&quot;  # Mixed Feelings

spark = SparkSession.builder.appName(&quot;AnimeSentimentPrediction&quot;).getOrCreate()

# 加载决策树模型和 CountVectorizer
#decision_tree = joblib.load(&quot;gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/decision_tree_model.pkl&quot;)
#count_vectorizer = joblib.load(&quot;gs://europe-west1-biganime-1990c8ad-bucket/biganime_pyspark/count_vectorizer.pkl&quot;)
decision_tree = joblib.load('/home/model/decision_tree_model.pkl')
count_vectorizer = joblib.load('/home/model/count_vectorizer.pkl')

# Kafka生产者配置
producer_conf = {
    'bootstrap.servers': 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092',
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'PLAIN',
    'sasl.username': '4GLFTYVSO5V7MJ6B',
    'sasl.password': '71JNm1voEHT/NBq8BtAVVLfs8nhmnfcyxHJFCLZLwFPjOfJ9rJ0On4DNnMfun97c',
    'key.serializer': StringSerializer('utf_8'),
    'value.serializer': StringSerializer('utf_8')
}
producer = SerializingProducer(producer_conf)

# Kitsu评论预测
for i in range(3):
    page_number = i + 1
    url = f&quot;https://kitsu.io/api/edge/anime?sort=-user_count&amp;page[limit]=20&amp;page[offset]={i*20}&quot;
    response = requests.get(url)

    if response.status_code != 200:
        print('Error with status code:', response.status_code)
        continue

    data = response.json()['data']

    for anime in data:
        anime_id = anime['id']
        url = f&quot;https://kitsu.io/api/edge/media-reactions?filter[animeId]={anime_id}&amp;page[limit]=5&quot;
        response = requests.get(url)

        if response.status_code != 200:
            print(f&quot;Error with status code for Anime ID {anime_id}: {response.status_code}&quot;)
            continue

        reactions = response.json()['data']

        # 对kitsu评论进行预测并将预测结果发送到Kafka
        for reaction in reactions:
            reaction_content = reaction['attributes']['reaction']
            sentiment_prediction = predict_sentiment(reaction_content)  # directly call the function here
            print(sentiment_prediction)

            # 将updatedAt作为key
            reaction_updatedAt = reaction['attributes']['updatedAt']
            reaction_key = get_timestamp_from_iso8601(reaction_updatedAt)

            # Create a JSON object that includes the key and value
            sentiment_json = json.dumps({&quot;timestamp&quot;: reaction_key, &quot;sentiment&quot;: sentiment_prediction})
            
            # Send the JSON object to Kafka with a dummy key (or you can leave the key=None)
            producer.produce(topic='prediction_topics', value=sentiment_json)  

# Ensure all messages are sent
producer.flush()
</code></pre>
<h3 id="可视化">可视化</h3>
<p>一切准备就绪，我们回到composer（airflow）控制台，点击打开airflow界面：</p>
<figure data-type="image" tabindex="14"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133045.png" alt="" loading="lazy"></figure>
<p>来到熟悉的界面，如果DAG编写没有问题，你就可以触发dag了。</p>
<figure data-type="image" tabindex="15"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133129.png" alt="" loading="lazy"></figure>
<p>最终我们的三个topic都在elk平台上被消费后，会出现在这里：</p>
<figure data-type="image" tabindex="16"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133248.png" alt="" loading="lazy"></figure>
<p>接下来就是创建dataview和使用dashboard进行可视化了。</p>
<p>左边拉到底选择stack management，再选择dataview，我这里创建了两个data view：</p>
<figure data-type="image" tabindex="17"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133505.png" alt="" loading="lazy"></figure>
<p>然后我们就可以去elk的dashboard进行可视化操作了。</p>
<p>如果有实时的效果，要如下设置，refresh设置为1s：</p>
<figure data-type="image" tabindex="18"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133704.png" alt="" loading="lazy"></figure>
<p>最后做一个展示：</p>
<figure data-type="image" tabindex="19"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230528133813.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据池与大数据分析可视化综合应用的探索]]></title>
        <id>https://gagumi.github.io/post/shu-ju-chi-yu-da-shu-ju-fen-xi-ke-shi-hua-zong-he-ying-yong-de-tan-suo/</id>
        <link href="https://gagumi.github.io/post/shu-ju-chi-yu-da-shu-ju-fen-xi-ke-shi-hua-zong-he-ying-yong-de-tan-suo/">
        </link>
        <updated>2023-05-24T15:47:20.000Z</updated>
        <summary type="html"><![CDATA[<p style="font-size:20px;">学校的一个project，要求完成一整套的大数据流程。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
]]></summary>
        <content type="html"><![CDATA[<p style="font-size:20px;">学校的一个project，要求完成一整套的大数据流程。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
<!-- more -->
<h3 id="前期思路">前期思路</h3>
<p>决定做四个可视化：</p>
<ul>
<li>
<p>实时评分跟踪</p>
</li>
<li>
<p>实时评论情绪分析</p>
</li>
<li>
<p>各个类别的动漫在不同时间段的发布数量</p>
</li>
<li>
<p>实时热度排行</p>
</li>
</ul>
<p>大致思路为：</p>
<ul>
<li>数据准备：</li>
</ul>
<p>探索MyAnimeList API和Kitsu API</p>
<ul>
<li>
<p>数据收集：</p>
<p>创建一个 Airflow 任务来定期调用 MyAnimeList API 和 Kitsu API，获取动漫数据。这个任务可能需要一个 Python 脚本来调用 API，并处理返回的数据。<br>
将获取的数据发送到 Kafka 中。这可以通过 Kafka 的 Python 客户端库完成，例如 Confluent Kafka Python。</p>
</li>
<li>
<p>实时数据处理：</p>
<p>创建一个 Airflow 任务来启动 Spark Streaming 作业，从 Kafka 中读取数据，并进行实时数据处理。这个 Spark Streaming 作业需要进行一些数据清洗和预处理步骤，例如去除无效的数据，或者转换数据的格式。</p>
</li>
<li>
<p>数据转换：</p>
<p>创建一个 Airflow 任务来启动 Spark 作业，进行数据转换。</p>
<p>这个 Spark 作业需要将数据转换为 Parquet 格式，或者将日期字段转换为 UTC 格式。</p>
</li>
<li>
<p>情绪分析：</p>
<p>创建一个 Airflow 任务来启动 Spark MLlib 作业，进行情绪分析。</p>
<p>这个任务可能需要使用一个预训练的模型，或者自己的模型，对用户评论进行情绪分析。</p>
</li>
<li>
<p>数据存储：</p>
<p>创建一个 Airflow 任务来将处理后的数据存储在 Elasticsearch 中。</p>
<p>需要创建一个 Elasticsearch 索引来存储数据。使用 Elasticsearch 的 Python 客户端库，例如 elasticsearch-py，来进行这个操作。</p>
</li>
<li>
<p>数据可视化：</p>
<p>在 Grafana 中创建一个或多个仪表盘来展示你的数据。这一步可能不需要在 Airflow 中进行，需要定期更新你的 Grafana 仪表盘。</p>
</li>
<li>
<p>数据分析：</p>
<p>根据需求和可视化结果，创建三个 Airflow 任务来启动 Spark  作业，进行进一步的数据分析（实时评分跟踪，各个类别的动漫在不同时间段的发布数量，实时热度排行）。</p>
</li>
</ul>
<h3 id="期望返回数据">期望返回数据</h3>
<ol>
<li>
<p><strong>动漫基础信息</strong>：这可能包括动漫的名称，描述，类别（例如，是电视剧、电影还是OVA），年份，季度，制作公司，导演等。</p>
</li>
<li>
<p><strong>用户评分</strong>：用户对动漫的评分通常是重要的分析指标。可能需要了解评分的范围（例如，是1-5，还是1-10），以及评分是如何计算的（例如，是平均评分，还是中位数评分）。</p>
</li>
<li>
<p><strong>用户评论</strong>：用户的文字评论可以用于情绪分析。注意，可能需要额外的数据处理步骤，例如文本清洗和预处理。</p>
</li>
<li>
<p><strong>动漫的观看数量或热度</strong>：这可能由某种指标表示，例如用户观看的次数，或者用户对动漫的喜欢程度</p>
</li>
<li>
<p><strong>时间戳</strong>：这是实时分析所必需的。时间戳可以帮助了解评论或评分是何时被提交的。</p>
</li>
</ol>
<h3 id="api探索myanimelist-api">API探索：MyAnimeList API</h3>
<p>查看 MyAnimeList 官方文档，发现其返回功能较少。这里决定采用https://api.jikan.moe/v4 API代为抓取信息。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230523100244.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230525175011.png" alt="" loading="lazy"></figure>
<p>在其中浏览，发现以上两个GET的返回值能够完成我们的预期任务。决定以狂赌之渊（ID：34933 姐姐踩我）为例，进行API探索。<br>
考虑到访问限制，预期只返回前五条评论。先做一个简单的尝试。</p>
<pre><code class="language-python">import requests

def get_anime_info_and_top_reviews(anime_id, review_count=5):
    # Define the base URL for the Jikan API
    base_url = &quot;https://api.jikan.moe/v4&quot;

    # Create the URLs for the specific anime and its reviews
    anime_url = f&quot;{base_url}/anime/{anime_id}/full&quot;
    review_url = f&quot;{base_url}/anime/{anime_id}/reviews&quot;

    # Send a GET request to the Jikan API for the anime info
    anime_response = requests.get(anime_url)

    # If the GET request is successful, the status code will be 200
    if anime_response.status_code == 200:
        # Get the JSON data from the response
        anime_data = anime_response.json()

        # Access the 'data' field in the response
        anime_info = anime_data[&quot;data&quot;]

        # Print the title, type, airing dates, score, and popularity of the anime
        print(f&quot;Title: {anime_info['title']}&quot;)
        print(f&quot;Type: {anime_info['type']}&quot;)
        print(f&quot;Aired: {anime_info['aired']}&quot;)
        print(f&quot;Score: {anime_info['score']}&quot;)
        print(f&quot;Popularity: {anime_info['popularity']}&quot;)

    # Send a GET request to the Jikan API for the anime reviews
    review_response = requests.get(review_url)

    # If the GET request is successful, the status code will be 200
    if review_response.status_code == 200:
        # Get the JSON data from the response
        review_data = review_response.json()

        # Access the 'data' field in the response
        reviews = review_data[&quot;data&quot;]

        # Limit the number of reviews to the specified review count
        reviews = reviews[:review_count]

        # Iterate over each review
        for review in reviews:
            # Print the username of the reviewer and the review text
            print(f&quot;\nUsername: {review['user']['username']}&quot;)
            print(f&quot;Review: {review['review']}&quot;)
            print(f&quot;Score: {review['score']}&quot;)
            print(f&quot;Date: {review['date']}&quot;)

# Test the function with the anime ID for Kakegurui (ID 34933)
get_anime_info_and_top_reviews(34933)

</code></pre>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230523100939.png" alt="" loading="lazy"></figure>
<p>我所需要的动漫信息的返回非常成功，接下来是评论，确实输出了评论的前五条（Jikan API 的 &quot;/reviews&quot; 端点返回的评论是按照某种方式排序的，虽然具体的排序规则在 API 文档中并未明确说明，离谱），尝试多次请求返回结果相同。</p>
<h3 id="有关myanimelist的思考">有关MyAnimeList的思考</h3>
<p>由于其top榜单更新时间为每两天更新一次，且没有历史数据，导致只能实现定期收集top榜单数据，评论数据。</p>
<p>那么我们的目标就演变成：</p>
<ul>
<li><strong>实时搜索你想要的番剧</strong>，返回其封面，Ranked ，Popularity，简介以及对最近五条评论进行情感分析。</li>
<li><strong>实时显示当前最受欢迎的动画</strong>，展示其封面，Ranked ，Popularity，简介以及对最近五条评论进行情感分析。</li>
<li><strong>动画类型的分布</strong>：可以统计库中所有动画的类型（如动作、冒险、悬疑等），并通过饼状图或条形图来显示各类型动画的分布。（通过选择时间段来进行改变）</li>
<li><strong>动画播放时段</strong>：分析和展示动画的播放时段，比如哪个季度或哪个月份播放的动画数量最多（通过选择季度）。</li>
<li>实时功能实现：每天获取两次TOP50榜单。</li>
<li>实时功能实现：定期地（例如每天或每小时）获取TOP50的最新评论，并将它们存储在自己的数据库中，附带获取评论的日期。然后，可以在数据库中查询特定时间段的评论。在 MyAnimeList 的评论数据中，<code>mal_id</code>字段是每条评论的唯一标识符。可以使用这个字段来检查新获取的评论是否已经在数据库中。</li>
</ul>
<h3 id="关于所有动画">关于所有动画</h3>
<p>TMD，由于Jikan API并不允许获取所有动画，所以只能使用<code>top</code>端点来获取前50名的动画，并从中提取类型信息。进行尝试。</p>
<p>这里需要注意，其TOP请求的参数如下例子：</p>
<pre><code class="language-python">params = {
    'type': 'tv',
    'filter': 'airing',
    'page': 1,
    'limit': 50
}
</code></pre>
<p>那么估计其请求应该为https://api.jikan.moe/v4/anime?filter=bypopularity&amp;page=1&amp;limit=50</p>
<p>拿到了结果，此时再通过每个动漫的mal_id来查找并获取资源</p>
<pre><code class="language-python">import time
import requests
from collections import defaultdict
import matplotlib.pyplot as plt

# 创建一个默认字典来存储每个类型的数量
genre_counts = defaultdict(int)

# 获取前50名的动画
response = requests.get(&quot;https://api.jikan.moe/v4/anime?filter=bypopularity&amp;page=1&amp;limit=50&quot;)
top_50_anime = response.json()

for anime in top_50_anime['data']:
    # 对每个动画发送一个请求，获取其详细信息
    response = requests.get(f&quot;https://api.jikan.moe/v4/anime/{anime['mal_id']}&quot;)
    anime_detail = response.json()

    # 遍历动画的所有类型，增加计数
    for genre in anime_detail['data']['genres']:
        genre_counts[genre['name']] += 1

    # 休眠1秒，以避免发送过多的请求
    time.sleep(1)

# 打印每个类型的数量
for genre, count in genre_counts.items():
    print(f&quot;{genre}: {count}&quot;)

# 准备数据
genres = list(genre_counts.keys())
counts = list(genre_counts.values())

# 创建条形图
plt.figure(figsize=(10,6))
plt.barh(genres, counts, color='skyblue')  # 使用水平条形图 barh，方便标签的显示

# 设置标题和标签
plt.title('Genre Distribution of Top 50 Anime')
plt.xlabel('Count')
plt.ylabel('Genre')

# 显示图表
plt.show()
</code></pre>
<p>其结果如下：</p>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/image-20230523125808624.png" alt="" loading="lazy"></figure>
<p>这样的结果与主页显示的内容相悖，需要重新选择url来发送请求。</p>
<p>https://api.jikan.moe/v4/top/anime?page=1<br>
虽然在官网上看起来一页有50个动漫，但是，经过观察返回值，可以发现，一页其实是25个动漫，那么，我们还需要遍历1 ，2。</p>
<pre><code class="language-python">import time
import requests
from collections import defaultdict
import matplotlib.pyplot as plt

# 创建一个默认字典来存储每个类型的数量
genre_counts = defaultdict(int)

# 获取前50名的动画，通过两次循环获取第1页和第2页的数据
for page in [1, 2]:
    response = requests.get(f&quot;https://api.jikan.moe/v4/top/anime?page={page}&quot;)
    top_anime = response.json()

    for anime in top_anime['data']:
        # 对每个动画发送一个请求，获取其详细信息
        response = requests.get(f&quot;https://api.jikan.moe/v4/anime/{anime['mal_id']}&quot;)
        anime_detail = response.json()

        # 打印动画的名字
        print(f&quot;Anime name: {anime_detail['data']['title']}&quot;)

        # 遍历动画的所有类型，增加计数
        for genre in anime_detail['data']['genres']:
            genre_counts[genre['name']] += 1

        # 休眠1秒，以避免发送过多的请求
        time.sleep(1)

# 打印每个类型的数量
for genre, count in genre_counts.items():
    print(f&quot;{genre}: {count}&quot;)

# 准备数据
genres = list(genre_counts.keys())
counts = list(genre_counts.values())

# 创建条形图
plt.figure(figsize=(10,6))
plt.barh(genres, counts, color='skyblue')  # 使用水平条形图 barh，方便标签的显示

# 设置标题和标签
plt.title('Genre Distribution of Top 50 Anime')
plt.xlabel('Count')
plt.ylabel('Genre')

# 显示图表
plt.show()
</code></pre>
<p>结果正常</p>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230523131848.png" alt="" loading="lazy"></figure>
<p>下面是统计图：</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230523132046.png" alt="" loading="lazy"></figure>
<h4 id="通过seson抓取动画名称等内容2006-2023">通过seson抓取动画名称等内容（2006-2023）</h4>
<p>将项目迁移到colab上，期望输出&quot;Year&quot;, &quot;Season&quot;, &quot;Anime Name&quot;, &quot;Score&quot;, &quot;MAL ID&quot;，进行试运行的时候出现问题，很多老日漫没有英文名字。</p>
<p>一开始使用罗马音翻译，出现问题且表意可能不准确，于是使用translation库，如果没有英文标题，将其翻译为日语。</p>
<p>顺便将得到的结果存入csv中。</p>
<pre><code class="language-python">import csv
import requests
import time
from translate import Translator

translator = Translator(from_lang='ja', to_lang='en')

years = range(2006, 2024)
seasons = ['winter', 'spring', 'summer', 'fall']
anime_data = []

for year in years:
    for season in seasons:
        response = requests.get(f&quot;https://api.jikan.moe/v4/seasons/{year}/{season}&quot;)
        data = response.json()
        last_page = data['pagination']['last_visible_page']
        for page in range(1, last_page + 1):
            response = requests.get(f&quot;https://api.jikan.moe/v4/seasons/{year}/{season}?page={page}&quot;)
            data = response.json()
            for anime in data['data']:
                anime_name = None
                anime_id = anime['mal_id']
                for title_info in anime['titles']:
                    if title_info['type'] == 'English':
                        anime_name = title_info['title']
                        break
                if anime_name is None:
                    # English title is missing, use translation
                    japanese_title = anime['title_japanese']
                    anime_name = translator.translate(japanese_title)
                anime_score = anime['score']
                print(f&quot;Year: {year}, Anime: {anime_name}, MAL ID: {anime_id}&quot;)
                anime_data.append([year, season, anime_name, anime_score, anime_id])
            time.sleep(1)

with open('anime_seasons.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow([&quot;Year&quot;, &quot;Season&quot;, &quot;Anime Name&quot;, &quot;Score&quot;, &quot;MAL ID&quot;])
    writer.writerows(anime_data)
</code></pre>
<p>抓取结果较差，有几率出现空的返回值。<br>
放弃翻译，没有英文标题的直接记录日语标题。</p>
<p>抓下来数据之后，准备工作完成。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[# 有关ELK的安装与使用]]></title>
        <id>https://gagumi.github.io/post/you-guan-elk-de-an-zhuang-yu-shi-yong/</id>
        <link href="https://gagumi.github.io/post/you-guan-elk-de-an-zhuang-yu-shi-yong/">
        </link>
        <updated>2023-05-14T20:30:08.000Z</updated>
        <summary type="html"><![CDATA[<p style="font-size:20px;">ELK 是一个开源软件堆栈，由三个核心组件组成：Elasticsearch、Logstash 和 Kibana。每个组件都具有不同的功能，共同构建了一个强大的日志管理和分析平台。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
]]></summary>
        <content type="html"><![CDATA[<p style="font-size:20px;">ELK 是一个开源软件堆栈，由三个核心组件组成：Elasticsearch、Logstash 和 Kibana。每个组件都具有不同的功能，共同构建了一个强大的日志管理和分析平台。
</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3" loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
<!-- more -->
<p>学校提供了四个月的使用资格，但是显示只显示14天。</p>
<p>以下是每个组件的简要介绍：</p>
<p>Elasticsearch：Elasticsearch 是一个实时分布式搜索和分析引擎，用于存储、搜索和分析大规模数据。它被设计用于高可扩展性和容错性，可以处理海量数据，并提供强大的全文搜索、复杂查询、聚合和分析能力。</p>
<p>Logstash：Logstash 是一个用于数据收集、处理和传输的开源工具。它可以从各种来源（如日志文件、数据库、消息队列等）收集数据，并将其进行转换、标准化和过滤，然后发送到不同的目标，其中包括 Elasticsearch。</p>
<p>Kibana：Kibana 是一个用于数据可视化和分析的开源工具。它提供了直观的用户界面，让用户能够通过交互式图表、仪表盘、地图等方式来探索和可视化 Elasticsearch 中的数据。Kibana 提供了丰富的可视化功能，使用户能够以直观的方式理解和分析数据。</p>
<p>这三个组件共同协作，构建了一个强大的日志管理和分析平台。Logstash 用于收集和处理数据，Elasticsearch 用于存储和索引数据，而 Kibana 则提供了交互式的数据可视化和分析界面。</p>
<p>ELK 被广泛应用于日志管理、实时监控、数据分析和业务智能等领域。它能够处理和分析各种类型的数据，包括日志数据、指标数据、文本数据等，帮助用户发现潜在的问题、获得实时见解，并支持数据驱动的决策和运营优化。</p>
<p>除了核心组件之外，ELK 生态系统还包括其他工具和插件，用于扩展和增强功能。例如，Beats 用于轻量级数据收集，Elastic APM 用于应用性能监控，Elasticsearch SQL 用于执行 SQL 查询等。</p>
<p>总而言之，ELK 提供了一个强大的平台，用于集中存储、搜索、分析和可视化各种类型的数据。它的灵活性和可扩展性使其成为处理大规模数据和解决实时分析挑战的理想选择。</p>
<h3 id="登录信息">登录信息</h3>
<p>在启动一个项目（test-lesson）之后，能获得账号密码</p>
<p>username,password<br>
elastic,gUhQRAqAalsPgBFXVxzGwkCV</p>
<h3 id="home-page">Home page</h3>
<p><a href="https://test-lesson-93537f.kb.us-central1.gcp.cloud.es.io:9243/app/integrations/browse">Browse integrations - Integrations - Elastic (es.io)</a></p>
<p>端口是9243</p>
<p>https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243</p>
<h3 id="注册-with-ubuntu">注册 with ubuntu</h3>
<pre><code class="language-ubuntu">curl --version
</code></pre>
<pre><code class="language-ubuntu">curl -X GET -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/_cat/indices?v'
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516171051.png" alt="" loading="lazy"></figure>
<h3 id="创建一个index">创建一个index</h3>
<pre><code>curl -X PUT 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer?pretty'
</code></pre>
<p>不会成功，需要添加身份验证（总是需要吗？）</p>
<pre><code class="language-ubuntu">curl -X PUT -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer?pretty'
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516171857.png" alt="" loading="lazy"></figure>
<pre><code class="language-ubuntu">curl -X GET -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/_cat/indices?v'
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516172240.png" alt="" loading="lazy"></figure>
<p>可以看到已经多了一个customer</p>
<pre><code>curl -X PUT -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer/_doc/1?pretty' -H 'Content-Type:application/json' -d '{\&quot;firstname\&quot;:\&quot;John\&quot;, \&quot;lastname\&quot;:\&quot;Doe\&quot;, \&quot;age\&quot;:22}' 
</code></pre>
<p>🤡，不需要转义字符捏</p>
<pre><code class="language-ubuntu">curl -X PUT -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer/_doc/1?pretty' -H 'Content-Type:application/json' -d '{&quot;firstname&quot;:&quot;John&quot;, &quot;lastname&quot;:&quot;Doe&quot;, &quot;age&quot;:22}'
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516173535.png" alt="" loading="lazy"></figure>
<pre><code class="language-ubuntu">curl -X PUT -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer/_doc/2?pretty' -H 'Content-Type:application/json' -d '{&quot;firstname&quot;:&quot;Anna&quot;, &quot;lastname&quot;:&quot;Conda&quot;, &quot;age&quot;:37}'
</code></pre>
<pre><code class="language-ubuntu">curl -X PUT -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer/_doc/3?pretty' -H 'Content-Type:application/json' -d '{&quot;firstname&quot;:&quot;Bob&quot;, &quot;lastname&quot;:&quot;Dylan&quot;, &quot;age&quot;:37}'
</code></pre>
<p>接下来使用query语句小检查一手</p>
<pre><code class="language-ubuntu">curl -X GET -u elastic:gUhQRAqAalsPgBFXVxzGwkCV 'https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243/customer/_doc/1?pretty'
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516174431.png" alt="" loading="lazy"></figure>
<p>或者使用云上（或是本地的）web界面的Dev  Tools 发送命令：</p>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516174913.png" alt="" loading="lazy"></figure>
<h3 id="可视化-in-kibana">可视化 in kibana</h3>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516180803.png" alt="" loading="lazy"></figure>
<p>创建一个名为“customer”的index pattern，在创建输入名字的时候，会自动匹配已有的资源。</p>
<p>实现四个filer(DSL query)</p>
<ul>
<li>
<p>firstname : Anna</p>
<pre><code class="language-sql">{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;firstname&quot;: &quot;Anna&quot;
    }
  }
}
</code></pre>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516183355.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>firstname : *n(找到任何以n结尾的)</p>
<pre><code class="language-sql">{
  &quot;wildcard&quot;: {
    &quot;firstname&quot;: &quot;*n&quot;
  }
}
</code></pre>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516183507.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>firstname : *n*</p>
<pre><code class="language-sql">{
  &quot;wildcard&quot;: {
    &quot;firstname&quot;: &quot;*n*&quot;
  }
}
</code></pre>
<figure data-type="image" tabindex="10"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/image-20230516183852518.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>age &gt; 23</p>
</li>
</ul>
<pre><code class="language-sql">{
  &quot;query&quot;: {
    &quot;range&quot;: {
      &quot;age&quot;: {
        &quot;gt&quot;: 23
      }
    }
  }
}
</code></pre>
<figure data-type="image" tabindex="11"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516184127.png" alt="" loading="lazy"></figure>
<p>除了 <code>gt</code>，Elasticsearch 的 range 查询还支持以下操作符：</p>
<ul>
<li><code>lt</code>：小于（less than）</li>
<li><code>gte</code>：大于或等于（greater than or equal to）</li>
<li><code>lte</code>：小于或等于（less than or equal to）</li>
</ul>
<h3 id="柱状图可视化age和count">柱状图可视化（age和count）</h3>
<figure data-type="image" tabindex="12"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516210732.png" alt="" loading="lazy"></figure>
<p>save as “customer per age” 并在dashboard上展示。</p>
<figure data-type="image" tabindex="13"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516211109.png" alt="" loading="lazy"></figure>
<h3 id="python-elastic">Python &amp; Elastic</h3>
<p>使用pip安装：</p>
<pre><code class="language-cmd">pip install elasticsearch
</code></pre>
<p>以kaggle数据集为例子进行试验：（https://www.kaggle.com/datasets/datasnaek/chess）</p>
<pre><code class="language-python">from datetime import datetime
import os
from elasticsearch import Elasticsearch,helpers

password = os.environ[&quot;ENV_PASSWORD&quot;]
client = Elasticsearch(hosts=[&quot;https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243&quot;],basic_auth=('elastic',password))  
docs = []
docs.append({&quot;name&quot;:&quot;Bob&quot;,&quot;timestamp&quot;:datetime.utcnow().isoformat()})

helpers.bulk(client,docs,index=&quot;index_fo_python_test&quot;)
</code></pre>
<p>一开始直接给<code>os.environ[&quot;ENV_PASSWORD&quot;]</code>里输入了我的密码，但这是不正确的，会报错key error，<code>password = os.environ[]</code> 这行代码的作用是从环境变量中获取一个值，并将其赋给变量 <code>password</code>。</p>
<p>在操作系统中，环境变量是一些全局的值，用于存储配置信息、认证凭据、路径等。通过使用 <code>os.environ</code>，你可以在 Python 脚本中访问这些环境变量。</p>
<p>具体来说，<code>os.environ</code> 是一个字典对象，它包含了当前系统环境中所有已定义的环境变量。你可以使用环境变量来存储敏感信息（如密码、API 密钥等），而不需要直接在代码中硬编码这些敏感信息。</p>
<p>所以需要在环境中设置其值：</p>
<pre><code class="language-cmd">set ENV_PASSWORD=gUhQRAqAalsPgBFXVxzGwkCV
</code></pre>
<p>再次运行python文件，通过，接下来去进行可视化</p>
<p>去往Stack Management页面，查看并创建一个data views，严格按照python代码里面的信息创建：</p>
<figure data-type="image" tabindex="14"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516214519.png" alt="" loading="lazy"></figure>
<p>回到discover，并选择index_fo_python_test</p>
<figure data-type="image" tabindex="15"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516214844.png" alt="" loading="lazy"></figure>
<h3 id="使用python进行实时发送">使用python进行实时发送</h3>
<p>我使用了以下的代码进行实时发送：</p>
<pre><code class="language-python">import datetime
import random
import time
import os
from elasticsearch import Elasticsearch, helpers

# 连接 Elasticsearch
password = os.environ[&quot;ENV_PASSWORD&quot;]
es = Elasticsearch(hosts=[&quot;https://test-lesson-93537f.es.us-central1.gcp.cloud.es.io:9243&quot;],basic_auth=('elastic',password))  
# 定义发送数据的函数
def send_data():
    # 生成随机数据
    data = {
        &quot;timestamp&quot;: datetime.datetime.utcnow(),
        &quot;value&quot;: random.randint(1, 100)
    }

    # 构建索引操作
    action = {
        &quot;_index&quot;: &quot;index_for_realtime_by_python&quot;,
        &quot;_source&quot;: data
    }

    # 批量索引数据
    helpers.bulk(es, [action])

# 发送数据并观察仪表盘变化
while True:
    # 发送数据到 Elasticsearch
    send_data()

    # 暂停5秒
    time.sleep(5)

</code></pre>
<p>在平台上进行可视化设置（别忘了新建一个dataview，同上）</p>
<figure data-type="image" tabindex="16"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516221022.png" alt="" loading="lazy"></figure>
<p>在dashboard上看起来这个图像是死的，你需要手动刷新。这时去到discover页面：右上角设置refresh every 1s（或是更短），此时图表就实时动起来力！</p>
<figure data-type="image" tabindex="17"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/20230516222304.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[写在最开始]]></title>
        <id>https://gagumi.github.io/post/xie-zai-zui-kai-shi/</id>
        <link href="https://gagumi.github.io/post/xie-zai-zui-kai-shi/">
        </link>
        <updated>2023-05-13T06:24:27.000Z</updated>
        <summary type="html"><![CDATA[<p style="font-size:20px;">差不多得了😅</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3"  loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
]]></summary>
        <content type="html"><![CDATA[<p style="font-size:20px;">差不多得了😅</p>
<audio controls src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/audio/HOYO-MiX%20_%20Anthony%20Lynch%20-%20%E8%B8%8F%E4%B8%8A%E6%97%85%E9%80%94%20Take%20the%20Journey.flac" type="audio/flac" volume="0.3"  loop>
  对不起，你的浏览器不支持 audio 元素。
</audio>
<!-- more -->
<p>天道酬勤，宁静致远，天知道哥们能坚持写这b博客多久捏。</p>
<p>不论如何先测试一下PicGo的图片。</p>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/Gagumi/MyPicGo/main/img/%E8%B0%A2%E8%8F%B2%E5%B0%94%E5%BE%B7(%CE%BC%E5%85%B5%E8%A3%85).png" alt="测试图片（谢菲尔德兵装）" loading="lazy"></figure>
<p>très bien！就这样吧，今天先到这儿，睡了😅</p>
]]></content>
    </entry>
</feed>